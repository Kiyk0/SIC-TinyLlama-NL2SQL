{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13853220,"sourceType":"datasetVersion","datasetId":8824667}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U \\\n    \"transformers>=4.38.0\" \\\n    \"datasets>=2.14.0\" \\\n    \"accelerate>=0.24.0\" \\\n    \"bitsandbytes>=0.41.0\" \\\n    \"peft>=0.7.0\" \\\n    \"protobuf==3.20.3\" \\\n    sentencepiece \\\n    evaluate","metadata":{"colab_type":"code","vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:30:28.581505Z","iopub.execute_input":"2025-11-24T16:30:28.581770Z","iopub.status.idle":"2025-11-24T16:30:33.056594Z","shell.execute_reply.started":"2025-11-24T16:30:28.581749Z","shell.execute_reply":"2025-11-24T16:30:33.055786Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from pathlib import Path\nimport os\nfrom pprint import pprint\nfrom typing import Optional\n\nproject_root = Path.cwd()\ndata_dir = project_root / \"data\"\noutput_dir = project_root / \"outputs\" / \"tinyllama-custom\"\ndata_dir.mkdir(parents=True, exist_ok=True)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Keep bitsandbytes on a single GPU to avoid DataParallel cuBLAS errors\nif \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    print(\"Pinned CUDA_VISIBLE_DEVICES=0 (override before this cell if you need multi-GPU).\")\nelse:\n    print(f\"Respecting existing CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}\")\n\ndef detect_kaggle_train_dev() -> Optional[Path]:\n    kaggle_input = Path(\"/kaggle/input\")\n    if not kaggle_input.exists():\n        return None\n    for dataset_dir in kaggle_input.iterdir():\n        if not dataset_dir.is_dir():\n            continue\n        train_path = dataset_dir / \"train.json\"\n        dev_path = dataset_dir / \"dev.json\"\n        if train_path.exists() and dev_path.exists():\n            return dataset_dir\n        # allow nested folders (e.g., uploaded zip extractions)\n        nested_train = list(dataset_dir.rglob(\"train.json\"))\n        for candidate in nested_train:\n            sibling_dev = candidate.parent / \"dev.json\"\n            if sibling_dev.exists():\n                return candidate.parent\n    return None\n\nenv_spider_root = os.getenv(\"SPIDER2_ROOT\")\nkaggle_spider_root = detect_kaggle_train_dev()\ndefault_spider_root = env_spider_root or (str(kaggle_spider_root) if kaggle_spider_root else str((project_root / \"Spider2\").resolve()))\n\nif kaggle_spider_root:\n    print(f\"Detected Kaggle train/dev files under: {kaggle_spider_root}\")\nelif env_spider_root:\n    print(f\"Using SPIDER2_ROOT from environment: {env_spider_root}\")\nelse:\n    print(\"Falling back to ./Spider2 (override with SPIDER2_ROOT if needed).\")\n\nconfig = {\n    \"base_model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    \"train_file\": str(data_dir / \"train.jsonl\"),\n    \"eval_file\": str(data_dir / \"eval.jsonl\"),\n    \"system_prompt\": \"You are TinyLlama, a compact and helpful assistant.\",\n    \"max_seq_length\": 512,\n    \"packing\": False,  # set True if you want to pack multiple samples per sequence\n    \"num_train_epochs\": 3,\n    \"per_device_train_batch_size\": 1,\n    \"per_device_eval_batch_size\": 1,\n    \"gradient_accumulation_steps\": 8,\n    \"learning_rate\": 2e-4,\n    \"warmup_ratio\": 0.03,\n    \"weight_decay\": 0.01,\n    \"logging_steps\": 10,\n    \"save_steps\": 200,\n    \"eval_steps\": 200,\n    \"max_grad_norm\": 1.0,\n    \"lr_scheduler_type\": \"cosine\",\n    \"lora_r\": 16,\n    \"lora_alpha\": 32,\n    \"lora_dropout\": 0.05,\n    \"lora_target_modules\": [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n    \"bf16\": False,\n    \"fp16\": True,\n    \"push_to_hub\": False,\n    \"hub_model_id\": \"\",  # fill this if push_to_hub is True\n    \"hub_token\": os.getenv(\"HUGGINGFACE_HUB_TOKEN\", \"\"),\n    \"output_dir\": str(output_dir),\n    # Spider 2.0 controls\n    \"use_spider2\": True,\n    \"spider2_root\": default_spider_root,\n    \"spider2_train_json\": \"train.json\",\n    \"spider2_dev_json\": \"dev.json\",\n    \"spider2_train_out\": str(data_dir / \"spider2-train.jsonl\"),\n    \"spider2_dev_out\": str(data_dir / \"spider2-dev.jsonl\"),\n    \"spider2_sample_limit\": None,  # set to an int for quick smoke tests\n}\n\nprint(\"Configuration summary:\")\npprint(config)","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:30:34.120118Z","iopub.execute_input":"2025-11-24T16:30:34.120880Z","iopub.status.idle":"2025-11-24T16:30:34.136078Z","shell.execute_reply.started":"2025-11-24T16:30:34.120846Z","shell.execute_reply":"2025-11-24T16:30:34.135462Z"}},"outputs":[{"name":"stdout","text":"Respecting existing CUDA_VISIBLE_DEVICES=0\nDetected Kaggle train/dev files under: /kaggle/input/nl2sql-from-spider2\nConfiguration summary:\n{'base_model_name': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n 'bf16': False,\n 'eval_file': '/kaggle/working/data/eval.jsonl',\n 'eval_steps': 200,\n 'fp16': True,\n 'gradient_accumulation_steps': 8,\n 'hub_model_id': '',\n 'hub_token': '',\n 'learning_rate': 0.0002,\n 'logging_steps': 10,\n 'lora_alpha': 32,\n 'lora_dropout': 0.05,\n 'lora_r': 16,\n 'lora_target_modules': ['q_proj',\n                         'k_proj',\n                         'v_proj',\n                         'o_proj',\n                         'gate_proj',\n                         'up_proj',\n                         'down_proj'],\n 'lr_scheduler_type': 'cosine',\n 'max_grad_norm': 1.0,\n 'max_seq_length': 512,\n 'num_train_epochs': 3,\n 'output_dir': '/kaggle/working/outputs/tinyllama-custom',\n 'packing': False,\n 'per_device_eval_batch_size': 1,\n 'per_device_train_batch_size': 1,\n 'push_to_hub': False,\n 'save_steps': 200,\n 'spider2_dev_json': 'dev.json',\n 'spider2_dev_out': '/kaggle/working/data/spider2-dev.jsonl',\n 'spider2_root': '/kaggle/input/nl2sql-from-spider2',\n 'spider2_sample_limit': None,\n 'spider2_train_json': 'train.json',\n 'spider2_train_out': '/kaggle/working/data/spider2-train.jsonl',\n 'system_prompt': 'You are TinyLlama, a compact and helpful assistant.',\n 'train_file': '/kaggle/working/data/train.jsonl',\n 'use_spider2': True,\n 'warmup_ratio': 0.03,\n 'weight_decay': 0.01}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from typing import Any, Dict, Optional\n\ndef serialize_schema(db_schema: Dict[str, Any]) -> str:\n    table_names = db_schema[\"table_names\"]\n    column_names = db_schema[\"column_names\"]\n    column_table_ids = db_schema[\"column_table_ids\"] if \"column_table_ids\" in db_schema else db_schema[\"column_names_original_table_ids\"]\n    lines = []\n    for table_index, table in enumerate(table_names):\n        cols = []\n        for (col_id, col), table_id in zip(enumerate(column_names), column_table_ids):\n            if table_id == table_index:\n                cols.append(col[1])\n        pretty_cols = \", \".join(cols) if cols else \"*\"\n        lines.append(f\"{table}({pretty_cols})\")\n    return \"\\n\".join(lines)\n\ndef make_instruction_sample(example: Dict[str, Any]) -> Dict[str, str]:\n    schema_text = serialize_schema(example[\"schema\"])\n    instruction = (\n        \"### Instruction:\\n\"\n        \"Given the following database schema, write the SQL query for the question.\\n\\n\"\n        \"### Schema:\\n\" + schema_text + \"\\n\\n\"\n        \"### Question:\\n\" + example[\"question\"] + \"\\n\\n\"\n        \"### SQL:\"\n    )\n    return {\n        \"instruction\": instruction,\n        \"response\": example[\"sql\"],\n        \"system\": \"You convert natural-language questions over relational databases into SQL queries.\",\n    }\n\ndef convert_spider_split(source_path: Path, target_path: Path, sample_limit: Optional[int] = None) -> None:\n    with source_path.open(\"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if sample_limit:\n        data = data[:sample_limit]\n    converted = [make_instruction_sample(row) for row in data]\n    with target_path.open(\"w\", encoding=\"utf-8\") as fw:\n        for record in converted:\n            fw.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n    print(f\"Saved {len(converted)} samples to {target_path}\")","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:30:36.753209Z","iopub.execute_input":"2025-11-24T16:30:36.753749Z","iopub.status.idle":"2025-11-24T16:30:36.760791Z","shell.execute_reply.started":"2025-11-24T16:30:36.753724Z","shell.execute_reply":"2025-11-24T16:30:36.760069Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\nif config.get(\"use_spider2\", False):\n    spider_root = Path(config[\"spider2_root\"])\n    assert spider_root.exists(), f\"Spider2 root not found at {spider_root}. Update config['spider2_root'].\"\n    train_src = spider_root / config[\"spider2_train_json\"]\n    dev_src = spider_root / config[\"spider2_dev_json\"]\n    missing = [p for p in (train_src, dev_src) if not p.exists()]\n    if missing:\n        missing_str = \", \".join(str(p) for p in missing)\n        raise FileNotFoundError(\n            f\"Missing Spider2 JSON files: {missing_str}. If you're on Kaggle, make sure your dataset includes train.json and dev.json at the root or adjust config['spider2_train_json']/['spider2_dev_json'].\"\n        )\n    train_out = Path(config[\"spider2_train_out\"])\n    dev_out = Path(config[\"spider2_dev_out\"])\n    convert_spider_split(train_src, train_out, config[\"spider2_sample_limit\"])\n    convert_spider_split(dev_src, dev_out, config[\"spider2_sample_limit\"])\n    config[\"train_file\"] = str(train_out)\n    config[\"eval_file\"] = str(dev_out)\n    print(\"Config updated to use Spider 2.0 JSONL files:\")\n    print(\"  train_file ->\", config[\"train_file\"])\n    print(\"  eval_file  ->\", config[\"eval_file\"])\nelse:\n    print(\"Skipping Spider 2.0 conversion (set config['use_spider2']=True to enable).\")","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:31:08.102244Z","iopub.execute_input":"2025-11-24T16:31:08.102521Z","iopub.status.idle":"2025-11-24T16:31:13.826533Z","shell.execute_reply.started":"2025-11-24T16:31:08.102497Z","shell.execute_reply":"2025-11-24T16:31:13.825904Z"}},"outputs":[{"name":"stdout","text":"Saved 12248 samples to /kaggle/working/data/spider2-train.jsonl\nSaved 1484 samples to /kaggle/working/data/spider2-dev.jsonl\nConfig updated to use Spider 2.0 JSONL files:\n  train_file -> /kaggle/working/data/spider2-train.jsonl\n  eval_file  -> /kaggle/working/data/spider2-dev.jsonl\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import random\nfrom typing import Dict, List\n\nimport torch\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling,\n    TrainingArguments,\n    Trainer,\n    logging\n)\nfrom peft import LoraConfig, get_peft_model\n\nlogging.set_verbosity_info()\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nset_seed(42)\n\ndef maybe_create_sample_data(train_path: str, eval_path: str) -> None:\n    \"\"\"Create a toy dataset so the notebook can run end-to-end if you don't have data yet.\"\"\"\n    if Path(train_path).exists() and Path(eval_path).exists():\n        return\n    sample_examples = [\n        {\n            \"instruction\": \"Explain why the sky appears blue.\",\n            \"response\": \"The atmosphere scatters sunlight so shorter blue wavelengths are seen from every direction.\",\n            \"system\": \"You are a concise science explainer.\",\n        },\n        {\n            \"instruction\": \"Give me three tips for learning Python.\",\n            \"response\": \"1. Practice daily. 2. Read other people's code. 3. Build tiny projects and iterate.\",\n            \"system\": \"Be upbeat and practical.\",\n        },\n    ]\n    Path(train_path).write_text(\"\\n\".join(json.dumps(e) for e in sample_examples), encoding=\"utf-8\")\n    Path(eval_path).write_text(\"\\n\".join(json.dumps(e) for e in sample_examples[:1]), encoding=\"utf-8\")\n    print(f\"Sample dataset written to {train_path} / {eval_path}\")\n\ndef format_chat(example: Dict, system_prompt: str) -> Dict:\n    messages: List[Dict[str, str]] = []\n    if example.get(\"system\") or system_prompt:\n        messages.append({\n            \"role\": \"system\",\n            \"content\": example.get(\"system\") or system_prompt,\n        })\n    messages.append({\"role\": \"user\", \"content\": example[\"instruction\"]})\n    messages.append({\"role\": \"assistant\", \"content\": example[\"response\"]})\n    example[\"messages\"] = messages\n    return example","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:31:17.725515Z","iopub.execute_input":"2025-11-24T16:31:17.725776Z","iopub.status.idle":"2025-11-24T16:32:06.591304Z","shell.execute_reply.started":"2025-11-24T16:31:17.725756Z","shell.execute_reply":"2025-11-24T16:32:06.590701Z"}},"outputs":[{"name":"stderr","text":"2025-11-24 16:31:33.452206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764001893.882463      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764001894.048484      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"if not Path(config[\"train_file\"]).exists():\n    maybe_create_sample_data(config[\"train_file\"], config[\"eval_file\"])\n\ndata_files = {\"train\": config[\"train_file\"]}\nif Path(config[\"eval_file\"]).exists():\n    data_files[\"validation\"] = config[\"eval_file\"]\nelse:\n    print(\"No eval file detected â€” validation metrics will be skipped.\")\n\ntrain_ext = Path(config[\"train_file\"]).suffix.lower()\nif train_ext in {\".json\", \".jsonl\"}:\n    dataset = load_dataset(\"json\", data_files=data_files)\nelif train_ext == \".csv\":\n    dataset = load_dataset(\"csv\", data_files=data_files)\nelif train_ext in {\".parquet\", \".pq\"}:\n    dataset = load_dataset(\"parquet\", data_files=data_files)\nelse:\n    raise ValueError(f\"Unsupported file extension: {train_ext}\")\n\ndataset = dataset.map(lambda ex: format_chat(ex, config[\"system_prompt\"]))\ndataset","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:32:09.791348Z","iopub.execute_input":"2025-11-24T16:32:09.792002Z","iopub.status.idle":"2025-11-24T16:32:11.433654Z","shell.execute_reply.started":"2025-11-24T16:32:09.791976Z","shell.execute_reply":"2025-11-24T16:32:11.432947Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872544685bbe403bb727f38669b01310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4402e2238c47faa8696738761eb755"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b4f3fed45b4b50b43f6d6917ea7244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1484 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c264421f5b2948d8b8e63732dd241af3"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'response', 'system', 'messages'],\n        num_rows: 12248\n    })\n    validation: Dataset({\n        features: ['instruction', 'response', 'system', 'messages'],\n        num_rows: 1484\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config[\"base_model_name\"], use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ndef add_text(example):\n    example[\"text\"] = tokenizer.apply_chat_template(\n        example[\"messages\"],\n        tokenize=False,\n        add_generation_prompt=False,\n    )\n    return example\n\ndataset = dataset.map(add_text)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\npadding_strategy = \"max_length\" if config[\"packing\"] else False\n\ndef tokenize_for_causal_lm(batch):\n    tokenized = tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=config[\"max_seq_length\"],\n        padding=padding_strategy,\n    )\n    return tokenized\n\ntokenized_datasets = dataset.map(\n    tokenize_for_causal_lm,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    desc=\"Tokenizing conversations\",\n)\n\nprint(\"Sample formatted conversation:\")\nprint(dataset[\"train\"][0][\"text\"][:200] + \"...\")\nprint(\"\\nTokenized sample ids:\")\nprint(tokenized_datasets[\"train\"][0][\"input_ids\"][:20])","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:32:14.366837Z","iopub.execute_input":"2025-11-24T16:32:14.367150Z","iopub.status.idle":"2025-11-24T16:32:25.328472Z","shell.execute_reply.started":"2025-11-24T16:32:14.367126Z","shell.execute_reply":"2025-11-24T16:32:25.327822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3693db228c5d4c258ca032cdff9989b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7d63db034614fb4ab9f4532c65d556d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9e3efd4f020489ba9081211008a5c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79622249ff8d41aeb6b10cad7e0a01ea"}},"metadata":{}},{"name":"stderr","text":"loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json\nloading file chat_template.jinja from cache at None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"980032df29df4de2bbc1f0f83a7032d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1484 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ff571eeeef4782b3a4476505093f2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing conversations:   0%|          | 0/12248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ec3097d8cd4c48b0d59457332d9933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing conversations:   0%|          | 0/1484 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb4157b95dc4b9ea0bde81685161979"}},"metadata":{}},{"name":"stdout","text":"Sample formatted conversation:\n<|system|>\nYou convert natural-language questions over relational databases into SQL queries.</s>\n<|user|>\n### Instruction:\nGiven the following database schema, write the SQL query for the question.\n\n...\n\nTokenized sample ids:\n[1, 529, 29989, 5205, 29989, 29958, 13, 3492, 3588, 5613, 29899, 11675, 5155, 975, 1104, 1288, 21218, 964, 3758, 9365]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16 if config[\"bf16\"] else torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config[\"base_model_name\"],\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n)\n\nlora_config = LoraConfig(\n    r=config[\"lora_r\"],\n    lora_alpha=config[\"lora_alpha\"],\n    target_modules=config[\"lora_target_modules\"],\n    lora_dropout=config[\"lora_dropout\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, lora_config)\n\nhas_validation_split = \"validation\" in tokenized_datasets\nsupports_eval_strategy = hasattr(TrainingArguments, \"__dataclass_fields__\") and \"evaluation_strategy\" in TrainingArguments.__dataclass_fields__\neval_strategy = \"steps\" if has_validation_split else \"no\"\n\ntraining_args_kwargs = dict(\n    output_dir=config[\"output_dir\"],\n    num_train_epochs=config[\"num_train_epochs\"],\n    per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n    per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n    learning_rate=config[\"learning_rate\"],\n    warmup_ratio=config[\"warmup_ratio\"],\n    weight_decay=config[\"weight_decay\"],\n    logging_steps=config[\"logging_steps\"],\n    save_steps=config[\"save_steps\"],\n    eval_steps=config[\"eval_steps\"],\n    max_grad_norm=config[\"max_grad_norm\"],\n    lr_scheduler_type=config[\"lr_scheduler_type\"],\n    fp16=config[\"fp16\"],\n    bf16=config[\"bf16\"],\n    report_to=[\"tensorboard\"],\n)\n\nif supports_eval_strategy:\n    training_args_kwargs[\"evaluation_strategy\"] = eval_strategy\nelse:\n    training_args_kwargs[\"do_eval\"] = has_validation_split\n\ntraining_args = TrainingArguments(**training_args_kwargs)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"] if has_validation_split else None,\n    data_collator=data_collator,\n)\n\npeft_model.print_trainable_parameters()","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:32:29.373091Z","iopub.execute_input":"2025-11-24T16:32:29.373574Z","iopub.status.idle":"2025-11-24T16:32:44.703116Z","shell.execute_reply.started":"2025-11-24T16:32:29.373549Z","shell.execute_reply":"2025-11-24T16:32:44.702311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a560321466421d9b0b900e63facf69"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nOverriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37df5c27b7e416fab65149811c69601"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2\n}\n\ntarget_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7baac04c5c84d66bbeb9758ae71d188"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\nCould not locate the custom_generate/generate.py inside TinyLlama/TinyLlama-1.1B-Chat-v1.0.\nPyTorch: setting up devices\nUsing auto half precision backend\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_result = trainer.train()\ntrainer.save_state()\ntrain_result","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T16:32:51.506029Z","iopub.execute_input":"2025-11-24T16:32:51.506360Z","iopub.status.idle":"2025-11-24T19:45:40.731665Z","shell.execute_reply.started":"2025-11-24T16:32:51.506339Z","shell.execute_reply":"2025-11-24T19:45:40.730976Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 12,248\n  Num Epochs = 3\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 8\n  Total optimization steps = 4,593\n  Number of trainable parameters = 12,615,680\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4593' max='4593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4593/4593 3:12:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.866000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.696100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.576300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.425500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.235900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.159400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.156500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.072600</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.872300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.833000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.810500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.670900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.541800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.501500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.396700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.393300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.328500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.266100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.207500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.197600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.151100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.163600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.108200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.121800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.110300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.056300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.061800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.049100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.048300</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.043200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.037500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.043200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.045800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.033200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.044800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.050400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.038300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.027100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.028900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.022700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.036700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.022600</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.031300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.026900</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.026100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.027800</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.031300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.052500</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.026700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.035000</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.029200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.022000</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.031500</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.019300</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.023400</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.028300</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.022600</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.030900</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.019900</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.017500</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.019000</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.016300</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.018400</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.022000</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.029300</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.028800</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.017900</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.017700</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.016500</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.016000</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>0.015900</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>0.016100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-200/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-200/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-400/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-400/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-600/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-600/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-600/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-800/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-800/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-800/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-1000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1000/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1000/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-1200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1200/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1200/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-1400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1400/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1400/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-1600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1600/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1600/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1600/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-1800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1800/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1800/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-1800/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-2000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2000/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2000/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-2200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2200/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2200/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-2400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2400/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2400/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-2600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2600/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2600/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2600/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-2800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2800/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2800/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-2800/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-3000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3000/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3000/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-3200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3200/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3200/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-3400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3400/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3400/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-3600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3600/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3600/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3600/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-3800\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3800/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3800/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-3800/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-4000\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4000/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4000/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-4200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4200/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4200/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4200/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-4400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4400/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4400/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4400/special_tokens_map.json\nSaving model checkpoint to /kaggle/working/outputs/tinyllama-custom/checkpoint-4593\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4593/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4593/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/checkpoint-4593/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4593, training_loss=0.05841092236344912, metrics={'train_runtime': 11568.6483, 'train_samples_per_second': 3.176, 'train_steps_per_second': 0.397, 'total_flos': 1.0784368789751808e+17, 'train_loss': 0.05841092236344912, 'epoch': 3.0})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.save_model(config[\"output_dir\"])\ntokenizer.save_pretrained(config[\"output_dir\"])\n\n#if config[\"push_to_hub\"] and config[\"hub_model_id\"]:\n#    trainer.push_to_hub()\n#    tokenizer.push_to_hub(config[\"hub_model_id\"])\n#    print(f\"Pushed to https://huggingface.co/{config['hub_model_id']}\")\n\n# Optional: merge the adapters into the base model for standalone deployment\n# from peft import PeftModel\n# base_model = AutoModelForCausalLM.from_pretrained(\n#     config[\"base_model_name\"],\n#     device_map=\"auto\",\n#     trust_remote_code=True,\n# )\n# peft_model = PeftModel.from_pretrained(base_model, config[\"output_dir\"])\n# merged_model = peft_model.merge_and_unload()\n# merged_dir = Path(config[\"output_dir\"]) / \"merged\"\n# merged_model.save_pretrained(merged_dir)\n# tokenizer.save_pretrained(merged_dir)\n# print(f\"Merged model saved to {merged_dir}\")","metadata":{"vscode":{"languageId":"python"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T19:46:33.778069Z","iopub.execute_input":"2025-11-24T19:46:33.778364Z","iopub.status.idle":"2025-11-24T19:46:34.151869Z","shell.execute_reply.started":"2025-11-24T19:46:33.778342Z","shell.execute_reply":"2025-11-24T19:46:34.150929Z"}},"outputs":[{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/outputs/tinyllama-custom\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/special_tokens_map.json\nchat template saved in /kaggle/working/outputs/tinyllama-custom/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/special_tokens_map.json\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/outputs/tinyllama-custom/tokenizer_config.json',\n '/kaggle/working/outputs/tinyllama-custom/special_tokens_map.json',\n '/kaggle/working/outputs/tinyllama-custom/chat_template.jinja',\n '/kaggle/working/outputs/tinyllama-custom/tokenizer.model',\n '/kaggle/working/outputs/tinyllama-custom/added_tokens.json',\n '/kaggle/working/outputs/tinyllama-custom/tokenizer.json')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"try:\n    from huggingface_hub import HfHubHTTPError  # newer versions expose it at the top level\nexcept ImportError:  # fall back for older installs\n    from huggingface_hub.utils import HfHubHTTPError\n\npush_to_hub = config.get(\"push_to_hub\", False)\nhub_model_id = config.get(\"hub_model_id\") or \"\"\nhub_token = config.get(\"hub_token\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n\nif push_to_hub and hub_model_id:\n    if not hub_token:\n        raise ValueError(\n            \"Hugging Face token missingâ€”set config['hub_token'] (not recommended for sharing) \"\n            \"or export HUGGINGFACE_HUB_TOKEN before running this cell.\"\n        )\n\n    try:\n        trainer.push_to_hub(token=hub_token)\n        tokenizer.push_to_hub(hub_model_id, token=hub_token)\n        print(f\"âœ… Pushed to https://huggingface.co/{hub_model_id}\")\n    except HfHubHTTPError as err:\n        print(\"âŒ Push to Hub failed. Double-check:\")\n        print(\"  â€¢ The token has 'read' + 'write' permissions and hasn't expired.\")\n        print(\n            \"  â€¢ You are authenticated as the owner/collaborator of\"\n            f\" '{hub_model_id}' (repo naming: username-or-org/model_name).\"\n        )\n        print(\"  â€¢ The repo already exists or you have permission to create it in that namespace.\")\n        print(\"  â€¢ There are no stray spaces/newlines in the token.\")\n        print(\"Original error ->\", err)\n        raise\nelse:\n    print(\"Push to Hub skipped (set config['push_to_hub']=True and supply hub_model_id/token).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T20:37:59.408340Z","iopub.execute_input":"2025-11-24T20:37:59.408946Z","iopub.status.idle":"2025-11-24T20:38:06.200809Z","shell.execute_reply.started":"2025-11-24T20:37:59.408923Z","shell.execute_reply":"2025-11-24T20:38:06.200097Z"}},"outputs":[{"name":"stderr","text":"Saving model checkpoint to /kaggle/working/outputs/tinyllama-custom\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"4.57.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in /kaggle/working/outputs/tinyllama-custom/chat_template.jinja\ntokenizer config file saved in /kaggle/working/outputs/tinyllama-custom/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/outputs/tinyllama-custom/special_tokens_map.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c2b1ad0d99430ab902bd413c045032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19a0028898047a6a90148833842ad81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4195132bddd94cd3ba82bc1c8476752d"}},"metadata":{}},{"name":"stderr","text":"chat template saved in /tmp/tmps0t25_x9/chat_template.jinja\ntokenizer config file saved in /tmp/tmps0t25_x9/tokenizer_config.json\nSpecial tokens file saved in /tmp/tmps0t25_x9/special_tokens_map.json\nUploading the following files to aliabohendy22/tinyllama_NL2SQL: special_tokens_map.json,tokenizer.json,tokenizer_config.json,chat_template.jinja,tokenizer.model,README.md\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d50e50b181b49179879cd6308e1a280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6737b16272464945b46d81eaad303762"}},"metadata":{}},{"name":"stdout","text":"âœ… Pushed to https://huggingface.co/aliabohendy22/tinyllama_NL2SQL\n","output_type":"stream"}],"execution_count":41}]}